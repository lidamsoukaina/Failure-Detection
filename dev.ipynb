{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure Identification In Customer Reviews\n",
    "\n",
    "This notebook contains the code used to fine-tune several transformer-based models.\n",
    "\n",
    "## 1. Import packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T17:00:43.175099Z",
     "iopub.status.busy": "2022-09-14T17:00:43.174684Z",
     "iopub.status.idle": "2022-09-14T17:00:44.693042Z",
     "shell.execute_reply": "2022-09-14T17:00:44.692472Z",
     "shell.execute_reply.started": "2022-09-14T17:00:43.175034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\n",
    "from torch import nn\n",
    "from transformers import pipeline\n",
    "\n",
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T17:00:48.892626Z",
     "iopub.status.busy": "2022-09-14T17:00:48.892130Z",
     "iopub.status.idle": "2022-09-14T17:00:48.922873Z",
     "shell.execute_reply": "2022-09-14T17:00:48.922355Z",
     "shell.execute_reply.started": "2022-09-14T17:00:48.892601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>tablet_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>stars</th>\n",
       "      <th>Failure class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big_dataset_1</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>Customer support wants to take the computer ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>TF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big_dataset_2</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>Overall, a good experience. Fast and responsiv...</td>\n",
       "      <td>3</td>\n",
       "      <td>TF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big_dataset_3</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>I got this Chromebook recent for a trip I need...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big_dataset_4</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>Just what I wanted\\n</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big_dataset_5</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>Good battery life, thin and lightweight, handy...</td>\n",
       "      <td>4</td>\n",
       "      <td>TF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>Big_dataset_1211</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>Way to go Asus!  This chromebook is so beautif...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>Big_dataset_1212</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>The very first Chromebook I ever owned was the...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>Big_dataset_1213</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>Snappy little chromebook!! It's light weight, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>Big_dataset_1214</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>This laptop is truly amazing. I spent a ton of...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>Big_dataset_1215</td>\n",
       "      <td>Big_dataset</td>\n",
       "      <td>I've already had to return one unit for replac...</td>\n",
       "      <td>4</td>\n",
       "      <td>TF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1215 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             review_id    tablet_id  \\\n",
       "0        Big_dataset_1  Big_dataset   \n",
       "1        Big_dataset_2  Big_dataset   \n",
       "2        Big_dataset_3  Big_dataset   \n",
       "3        Big_dataset_4  Big_dataset   \n",
       "4        Big_dataset_5  Big_dataset   \n",
       "...                ...          ...   \n",
       "1210  Big_dataset_1211  Big_dataset   \n",
       "1211  Big_dataset_1212  Big_dataset   \n",
       "1212  Big_dataset_1213  Big_dataset   \n",
       "1213  Big_dataset_1214  Big_dataset   \n",
       "1214  Big_dataset_1215  Big_dataset   \n",
       "\n",
       "                                                comment  stars Failure class  \n",
       "0     Customer support wants to take the computer ba...      1            TF  \n",
       "1     Overall, a good experience. Fast and responsiv...      3            TF  \n",
       "2     I got this Chromebook recent for a trip I need...      5           NaN  \n",
       "3                      Just what I wanted\\n                  5           NaN  \n",
       "4     Good battery life, thin and lightweight, handy...      4            TF  \n",
       "...                                                 ...    ...           ...  \n",
       "1210  Way to go Asus!  This chromebook is so beautif...      5           NaN  \n",
       "1211  The very first Chromebook I ever owned was the...      4           NaN  \n",
       "1212  Snappy little chromebook!! It's light weight, ...      5           NaN  \n",
       "1213  This laptop is truly amazing. I spent a ton of...      5           NaN  \n",
       "1214  I've already had to return one unit for replac...      4            TF  \n",
       "\n",
       "[1215 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data as data frames\n",
    "df_train = pd.read_csv(\"training_data.csv\")\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T17:00:50.040871Z",
     "iopub.status.busy": "2022-09-14T17:00:50.040502Z",
     "iopub.status.idle": "2022-09-14T17:00:50.205218Z",
     "shell.execute_reply": "2022-09-14T17:00:50.204725Z",
     "shell.execute_reply.started": "2022-09-14T17:00:50.040847Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ef919baa748c99f1\n",
      "Found cached dataset csv (/home/soukaina/.cache/huggingface/datasets/csv/default-ef919baa748c99f1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 3/3 [00:00<00:00, 213.19it/s]\n",
      "Loading cached processed dataset at /home/soukaina/.cache/huggingface/datasets/csv/default-ef919baa748c99f1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d1f15d554317dd5e.arrow\n",
      "Loading cached processed dataset at /home/soukaina/.cache/huggingface/datasets/csv/default-ef919baa748c99f1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8f6efcba5ff496ee.arrow\n",
      "Loading cached processed dataset at /home/soukaina/.cache/huggingface/datasets/csv/default-ef919baa748c99f1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5e799483a3516507.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'tablet_id', 'comment', 'stars', 'Failure class', 'label'],\n",
       "        num_rows: 1215\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['review_id', 'tablet_id', 'comment', 'stars', 'Failure class', 'label'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'tablet_id', 'comment', 'stars', 'Failure class', 'label'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data as huggingface datasets\n",
    "ds = datasets.load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"training_data.csv\",\n",
    "        \"val\": \"validation_data.csv\",\n",
    "        \"test\": \"test_data.csv\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add a column for the label (1=failure, 0=non-failure)\n",
    "def add_label(data):\n",
    "    return {\n",
    "        \"label\": [int(c in [\"TF\", \"IF\"]) for c in data[\"Failure class\"]],\n",
    "    }\n",
    "ds = ds.map(add_label, batched=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper functions\n",
    "\n",
    "Here, we define functions which will help use finetune and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T17:00:54.366953Z",
     "iopub.status.busy": "2022-09-14T17:00:54.366568Z",
     "iopub.status.idle": "2022-09-14T17:00:54.986906Z",
     "shell.execute_reply": "2022-09-14T17:00:54.986309Z",
     "shell.execute_reply.started": "2022-09-14T17:00:54.366928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a set of pretrained weights for the distilbert transformer model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T17:00:59.144604Z",
     "iopub.status.busy": "2022-09-14T17:00:59.144228Z",
     "iopub.status.idle": "2022-09-14T17:00:59.467964Z",
     "shell.execute_reply": "2022-09-14T17:00:59.467391Z",
     "shell.execute_reply.started": "2022-09-14T17:00:59.144579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.56MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the balanced accuracy metric in a manner compatible with hugginface's\n",
    "# `evaluate` library\n",
    "class BalancedAccuracy(evaluate.EvaluationModule):\n",
    "    def _info(self):\n",
    "        return evaluate.MetricInfo(\n",
    "            description=\"\",\n",
    "            citation=\"\",\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"predictions\": datasets.Value(\"int32\"),\n",
    "                    \"references\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references):\n",
    "        return {\n",
    "            \"balanced_accuracy\": float(\n",
    "                balanced_accuracy_score(references, predictions)\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "# The metrics which we'll use to evaluate the models: accuracy, f1, balanced accuracy\n",
    "metrics = evaluate.combine([\"accuracy\", \"f1\", BalancedAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T17:01:01.508526Z",
     "iopub.status.busy": "2022-09-14T17:01:01.507959Z",
     "iopub.status.idle": "2022-09-14T17:01:01.523686Z",
     "shell.execute_reply": "2022-09-14T17:01:01.523158Z",
     "shell.execute_reply.started": "2022-09-14T17:01:01.508502Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes the metrics given the model output\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def run_training(\n",
    "    model,\n",
    "    *,\n",
    "    output_dir,\n",
    "    ds_,\n",
    "    tokenizer,\n",
    "    data_collator,\n",
    "    lr=2e-5,\n",
    "    batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    epochs=6,\n",
    "    make_trainer=Trainer,\n",
    "):\n",
    "    \"\"\"Fine-tunes a model on the training set with the specified data\n",
    "    and validates it on the validation set.\"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        log_level=\"warning\",\n",
    "        logging_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "    )\n",
    "    trainer = make_trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_[\"train\"],\n",
    "        eval_dataset=ds_[\"val\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning different models\n",
    "\n",
    "For each model, we ran started with a pretrained model and fine-tuned it on our dataset.\n",
    "We ran each model 5 times to take into account the possible variance of the training process.\n",
    "\n",
    "### 3.1. Distilbert model (dirty version)\n",
    "\n",
    "- Pretrained model name: `distilbert-base-uncased`\n",
    "- Link: <https://huggingface.co/distilbert-base-uncased>\n",
    "- Preprocessing:\n",
    "  - Strip spaces\n",
    "  - During the tokenization process, the review is truncated to 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T09:45:54.589150Z",
     "iopub.status.busy": "2022-09-14T09:45:54.588964Z",
     "iopub.status.idle": "2022-09-14T09:45:59.709692Z",
     "shell.execute_reply": "2022-09-14T09:45:59.708751Z",
     "shell.execute_reply.started": "2022-09-14T09:45:54.589133Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_comment(c: str) -> str:\n",
    "    return c.strip()\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    comments = list(map(preprocess_comment, data[\"comment\"]))\n",
    "    return {\n",
    "        **tokenizer(comments, truncation=True),\n",
    "        \"label\": [int(c in [\"TF\", \"IF\"]) for c in data[\"Failure class\"]],\n",
    "    }\n",
    "\n",
    "\n",
    "ds_base_noclean = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "for i in range(5):\n",
    "    run_training(\n",
    "        model,\n",
    "        output_dir=f\"./results-base-noclean{i}\",\n",
    "        ds_=ds_base_noclean,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Distilbert model (clean version)\n",
    "\n",
    "- Pretrained model name: `distilbert-base-uncased`\n",
    "- Link: <https://huggingface.co/distilbert-base-uncased>\n",
    "- Preprocessing:\n",
    "  - Remove the string \\_x000D\\_ present at the start end end of some reviews\n",
    "  - Strip spaces\n",
    "  - During the tokenization process, the review is truncated to 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T00:51:23.390588Z",
     "iopub.status.busy": "2022-09-14T00:51:23.390296Z",
     "iopub.status.idle": "2022-09-14T01:04:03.644429Z",
     "shell.execute_reply": "2022-09-14T01:04:03.643819Z",
     "shell.execute_reply.started": "2022-09-14T00:51:23.390568Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_comment(c: str) -> str:\n",
    "    return c.replace(\"_x000D_\", \"\").strip()\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    comments = list(map(preprocess_comment, data[\"comment\"]))\n",
    "    return {\n",
    "        **tokenizer(comments, truncation=True),\n",
    "        \"label\": [int(c in [\"TF\", \"IF\"]) for c in data[\"Failure class\"]],\n",
    "    }\n",
    "\n",
    "\n",
    "ds_base = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "for i in range(5):\n",
    "    run_training(\n",
    "        model,\n",
    "        output_dir=f\"./results-base{i}\",\n",
    "        ds_=ds_base,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Distilbert with score\n",
    "\n",
    "- Pretrained model name: `distilbert-base-uncased`\n",
    "- Link: <https://huggingface.co/distilbert-base-uncased>\n",
    "- Preprocessing:\n",
    "  - Remove the string \\_x000D\\_ present at the start end end of some reviews\n",
    "  - Strip spaces\n",
    "  - Add the score to the review text. For example, the review \"I like this product\" with 4 stars becomes \"Score: 4. I like this product\".\n",
    "  - During the tokenization process, the review is truncated to 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T01:04:03.645883Z",
     "iopub.status.busy": "2022-09-14T01:04:03.645592Z",
     "iopub.status.idle": "2022-09-14T01:16:42.578939Z",
     "shell.execute_reply": "2022-09-14T01:16:42.578322Z",
     "shell.execute_reply.started": "2022-09-14T01:04:03.645863Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_comment(c: str, stars: int) -> str:\n",
    "    c = c.replace(\"_x000D_\", \"\").strip()\n",
    "    return f\"Score: {stars}. {c}\"\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    comments = [\n",
    "        preprocess_comment(c, stars) for c, stars in zip(data[\"comment\"], data[\"stars\"])\n",
    "    ]\n",
    "    return {\n",
    "        **tokenizer(comments, truncation=True),\n",
    "        \"label\": [int(c in [\"TF\", \"IF\"]) for c in data[\"Failure class\"]],\n",
    "    }\n",
    "\n",
    "\n",
    "ds_score = ds.map(preprocess_function, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "for i in range(5):\n",
    "    run_training(\n",
    "        model,\n",
    "        output_dir=f\"./results-score{i}\",\n",
    "        ds_=ds_score,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Distilbert with truncation at the middle of the review (with score)\n",
    "\n",
    "- Pretrained model name: `distilbert-base-uncased`\n",
    "- Link: <https://huggingface.co/distilbert-base-uncased>\n",
    "- Preprocessing:\n",
    "  - Remove the string \\_x000D\\_ present at the start end end of some reviews\n",
    "  - Strip spaces\n",
    "  - Add the score to the review text. For example, the review \"I like this product\" with 4 stars becomes \"Score: 4. I like this product\".\n",
    "  - Before the tokenization proocess, we split the text in sentences and remove sentences from the middle of the review as needed to ensure that the number of tokens is at most 512. In particular:\n",
    "    - We always remove whole sentences, never single words\n",
    "    - We keep the start and end of each review, which appear (based on a quick manual analysis of the reviews) to be the most important parts of a review\n",
    "- Notes:\n",
    "  - For our dataset, removing the middle of a review yields worse performance that removing the end of a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T01:32:28.580641Z",
     "iopub.status.busy": "2022-09-14T01:32:28.580256Z",
     "iopub.status.idle": "2022-09-14T02:03:24.450493Z",
     "shell.execute_reply": "2022-09-14T02:03:24.449777Z",
     "shell.execute_reply.started": "2022-09-14T01:32:28.580617Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_comment(c: str, stars: int) -> str:\n",
    "    c = c.replace(\"_x000D_\", \"\").strip()\n",
    "    sentences = c.split(\".\")\n",
    "    start = sentences[:len(sentences) // 2]\n",
    "    end = sentences[len(sentences) // 2:]\n",
    "\n",
    "    while True:\n",
    "        text = f'Score: {stars}. ' + '.'.join(start + end)\n",
    "        if len(tokenizer(text, return_attention_mask=False, verbose=False)['input_ids']) <= 512:\n",
    "            return text\n",
    "\n",
    "        if len(start) > len(end):\n",
    "            start.pop()\n",
    "        else:\n",
    "            end.pop(0)\n",
    "\n",
    "def preprocess_function(data):\n",
    "    comments = [preprocess_comment(c, stars) for c, stars in zip(data[\"comment\"], data[\"stars\"])]\n",
    "    return {\n",
    "        **tokenizer(comments, truncation=True),\n",
    "        \"label\": [int(c in [\"TF\", \"IF\"]) for c in data[\"Failure class\"]],\n",
    "    }\n",
    "    \n",
    "ds_truncate_middle = ds.map(preprocess_function, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "for i in range(5):\n",
    "    run_training(\n",
    "        model,\n",
    "        output_dir=f\"./results-truncate-middle-score{i}\",\n",
    "        ds_=ds_truncate_middle,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Roberta with score\n",
    "\n",
    "- Pretrained model name: `roberta-base`\n",
    "- Link: <https://huggingface.co/roberta-base>\n",
    "- Preprocessing:\n",
    "  - Remove the string \\_x000D\\_ present at the start end end of some reviews\n",
    "  - Strip spaces\n",
    "  - Add the score to the review text. For example, the review \"I like this product\" with 4 stars becomes \"Score: 4. I like this product\".\n",
    "  - During the tokenization process, the review is truncated to 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T08:04:04.242713Z",
     "iopub.status.busy": "2022-09-14T08:04:04.242315Z",
     "iopub.status.idle": "2022-09-14T08:25:18.059894Z",
     "shell.execute_reply": "2022-09-14T08:25:18.058997Z",
     "shell.execute_reply.started": "2022-09-14T08:04:04.242689Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_comment(c: str, stars: int) -> str:\n",
    "    c = c.replace(\"_x000D_\", \"\").strip()\n",
    "    return f\"Score: {stars}. {c}\"\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    comments = [\n",
    "        preprocess_comment(c, stars) for c, stars in zip(data[\"comment\"], data[\"stars\"])\n",
    "    ]\n",
    "    return {\n",
    "        **tokenizer_roberta(comments, truncation=True),\n",
    "        \"label\": [int(c in [\"TF\", \"IF\"]) for c in data[\"Failure class\"]],\n",
    "    }\n",
    "\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "data_collator_roberta = DataCollatorWithPadding(tokenizer=tokenizer_roberta)\n",
    "\n",
    "ds_score = ds.map(preprocess_function, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=2\n",
    ")\n",
    "for i in range(2, 5):\n",
    "    run_training(\n",
    "        model,\n",
    "        output_dir=f\"./results-roberta{i}\",\n",
    "        ds_=ds_score,\n",
    "        tokenizer=tokenizer_roberta,\n",
    "        data_collator=data_collator_roberta,\n",
    "        batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Roberta for sentiment analysis with score\n",
    "\n",
    "- Pretrained model name: `cardiffnlp/twitter-roberta-base-sentiment-latest`\n",
    "  - A version of the roberta model which has been pretrained on tweets and fine-tuned for sentiment analysis\n",
    "- Link: <https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest>\n",
    "- Preprocessing:\n",
    "  - Remove the string \\_x000D\\_ present at the start end end of some reviews\n",
    "  - Strip spaces\n",
    "  - Add the score to the review text. For example, the review \"I like this product\" with 4 stars becomes \"Score: 4. I like this product\".\n",
    "  - During the tokenization process, the review is truncated to 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T15:30:31.484473Z",
     "iopub.status.busy": "2022-09-14T15:30:31.484085Z",
     "iopub.status.idle": "2022-09-14T15:33:33.976041Z",
     "shell.execute_reply": "2022-09-14T15:33:33.975189Z",
     "shell.execute_reply.started": "2022-09-14T15:30:31.484447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_comment(c: str, stars: int) -> str:\n",
    "    c = c.replace(\"_x000D_\", \"\").strip()\n",
    "    return f\"Score: {stars}. {c}\"\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    comments = [\n",
    "        preprocess_comment(c, stars) for c, stars in zip(data[\"comment\"], data[\"stars\"])\n",
    "    ]\n",
    "    return {\n",
    "        **tokenizer_roberta_sent(comments, truncation=True, max_length=512),\n",
    "        \"label\": [int(c in [\"TF\", \"IF\"]) for c in data[\"Failure class\"]],\n",
    "    }\n",
    "\n",
    "\n",
    "tokenizer_roberta_sent = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "data_collator_roberta_sent = DataCollatorWithPadding(tokenizer=tokenizer_roberta_sent)\n",
    "ds_score = ds.map(preprocess_function, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\", num_labels=2,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "for i in range(3, 5):\n",
    "    run_training(\n",
    "        model,\n",
    "        output_dir=f\"./results-roberta-sent{i}\",\n",
    "        ds_=ds_score,\n",
    "        tokenizer=tokenizer_roberta_sent,\n",
    "        data_collator=data_collator_roberta_sent,\n",
    "        batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Model\n",
    "\n",
    "The best model was [3.6. Roberta for sentiment analysis with score](#36-roberta-for-sentiment-analysis-with-score). We trained this model on the whole combined train+validation dataset and used it to generate predictions for the test data (evaluation.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T17:07:41.150374Z",
     "iopub.status.busy": "2022-09-14T17:07:41.149993Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrain the model on the combined train+validation data\n",
    "\n",
    "def preprocess_comment(c: str, stars: int) -> str:\n",
    "    c = c.replace(\"_x000D_\", \"\").strip()\n",
    "    return f\"Score: {stars}. {c}\"\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    comments = [\n",
    "        preprocess_comment(c, stars) for c, stars in zip(data[\"comment\"], data[\"stars\"])\n",
    "    ]\n",
    "    return {\n",
    "        **tokenizer_roberta_sent(comments, truncation=True, max_length=512),\n",
    "        \"label\": [int(c in [\"TF\", \"IF\"]) for c in data[\"Failure class\"]],\n",
    "    }\n",
    "\n",
    "\n",
    "tokenizer_roberta_sent = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "data_collator_roberta_sent = DataCollatorWithPadding(tokenizer=tokenizer_roberta_sent)\n",
    "ds_score = ds.map(preprocess_function, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\", num_labels=2,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "ds_score['train'] = datasets.concatenate_datasets([ds_score['train'], ds_score['val']])\n",
    "ds_score['val'] = ds_score['train']\n",
    "run_training(\n",
    "    model,\n",
    "    output_dir=f\"./results-roberta-full\",\n",
    "    ds_=ds_score,\n",
    "    tokenizer=tokenizer_roberta_sent,\n",
    "    data_collator=data_collator_roberta_sent,\n",
    "    epochs=6,\n",
    "    batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T18:52:00.173442Z",
     "iopub.status.busy": "2022-09-14T18:52:00.173061Z",
     "iopub.status.idle": "2022-09-14T18:52:01.182764Z",
     "shell.execute_reply": "2022-09-14T18:52:01.181940Z",
     "shell.execute_reply.started": "2022-09-14T18:52:00.173418Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a classification pipeline for easy prediction\n",
    "clf = pipeline('text-classification', model='results-roberta-full/checkpoint-336')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T19:05:41.510797Z",
     "iopub.status.busy": "2022-09-14T19:05:41.510411Z",
     "iopub.status.idle": "2022-09-14T19:05:41.551081Z",
     "shell.execute_reply": "2022-09-14T19:05:41.550535Z",
     "shell.execute_reply.started": "2022-09-14T19:05:41.510773Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2171bbe729724cbba6d663e56b1eba7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess the test reviews\n",
    "\n",
    "def preprocess_comment(c: str, stars: int) -> str:\n",
    "    c = c.replace(\"_x000D_\", \"\").strip()\n",
    "    return f\"Score: {stars}. {c}\"\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    return {\n",
    "        'preprocessed_comment': [preprocess_comment(c, stars) for c, stars in zip(data[\"comment\"], data[\"stars\"])]\n",
    "    }\n",
    "\n",
    "ds_test = ds['test'].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T19:20:51.245189Z",
     "iopub.status.busy": "2022-09-14T19:20:51.244815Z",
     "iopub.status.idle": "2022-09-14T19:21:13.263094Z",
     "shell.execute_reply": "2022-09-14T19:21:13.262460Z",
     "shell.execute_reply.started": "2022-09-14T19:20:51.245164Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7d3a447d7e43e0a92853fc2a12aaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review_id', 'tablet_id', 'comment', 'stars', 'Failure class', 'label', 'preprocessed_comment', 'prob_failure', 'pred_label'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the model predictions (probabilities)\n",
    "\n",
    "def generate_probs(data):\n",
    "    results = clf(data['preprocessed_comment'], truncation=True, max_length=512)\n",
    "    probs = []\n",
    "    for row in results:\n",
    "        if row['label'] == 'LABEL_0':\n",
    "            probs.append(1-row['score'])\n",
    "        elif row['label'] == 'LABEL_1':\n",
    "            probs.append(row['score'])\n",
    "        else:\n",
    "            raise ValueError('got label == ' + str(row['label']))\n",
    "    return {'prob_failure': probs, 'pred_label': [int(round(p)) for p in probs]}\n",
    "\n",
    "ds_test = ds_test.map(generate_probs, batched=True)\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T19:29:41.011402Z",
     "iopub.status.busy": "2022-09-14T19:29:41.011023Z",
     "iopub.status.idle": "2022-09-14T19:29:41.015394Z",
     "shell.execute_reply": "2022-09-14T19:29:41.014845Z",
     "shell.execute_reply.started": "2022-09-14T19:29:41.011378Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the labels\n",
    "\n",
    "with open('evaluation_predictions.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, ds_test['pred_label'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FailureDetection",
   "language": "python",
   "name": "failuredetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
